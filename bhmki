import os
import cv2
import io
import numpy as np
from PIL import Image, ImageDraw, ImageFont
from deskew import determine_skew
import fitz  # PyMuPDF
import concurrent.futures
import torch
from transformers import DetrImageProcessor, DetrForObjectDetection
import pytesseract

class PreprocessAgent:
    def __init__(self):
        self.layout_processor = DetrImageProcessor.from_pretrained("cmarkea/detr-layout-detection")
        self.layout_model = DetrForObjectDetection.from_pretrained("cmarkea/detr-layout-detection")
        try:
            self.font_path = "arial.ttf"
            self.font = ImageFont.truetype(self.font_path, 24)
        except:
            self.font_path = None
            self.font = ImageFont.load_default()

    def normalize_document(self, input_path):
        ext = os.path.splitext(input_path)[1].lower()
        handlers = {
            '.pdf': self._handle_pdf,
            '.txt': self._handle_txt,
            '.docx': self._handle_docx,
            '.jpg': self._handle_image,
            '.jpeg': self._handle_image,
            '.png': self._handle_image,
            '.bmp': self._handle_image,
            '.tiff': self._handle_image
        }
        if ext in handlers:
            return handlers[ext](input_path)
        raise ValueError(f"Unsupported file type: {ext}")

    def _handle_pdf(self, path):
        doc = fitz.open(path)
        return [{'image': Image.frombytes("RGB", [p.get_pixmap(dpi=300).width, p.get_pixmap(dpi=300).height], p.get_pixmap(dpi=300).samples), 'page': i+1} for i, p in enumerate(doc)]

    def _handle_image(self, path):
        img = Image.open(path).convert('RGB')
        img.thumbnail((2480, 2480), Image.LANCZOS)
        return [{'image': img, 'page': 1}]

    def _handle_txt(self, path):
        with open(path, 'r', encoding='utf-8') as f:
            text = f.read()
        return self._text_to_images(text)

    def _handle_docx(self, path):
        from docx import Document
        doc = Document(path)
        text = '\n'.join([p.text for p in doc.paragraphs])
        images = self._text_to_images(text)
        images += self.extract_docx_images(doc)
        return images

    def _text_to_images(self, text, width=2480, height=3508, font_size=24, max_lines=120):
        lines = text.split('\n')
        pages = []
        for i in range(0, len(lines), max_lines):
            img = Image.new("RGB", (width, height), "white")
            draw = ImageDraw.Draw(img)
            margin, offset = 50, 50
            for line in lines[i:i + max_lines]:
                draw.text((margin, offset), line, font=self.font, fill="black")
                offset += font_size + 10
                if offset > height - margin:
                    break
            pages.append({'image': img, 'page': (i // max_lines) + 1})
        return pages

    def extract_docx_images(self, doc):
        images = []
        for rel in doc.part.rels.values():
            if "image" in rel.target_ref:
                img = Image.open(io.BytesIO(rel.target_part.blob)).convert('RGB')
                images.append({'image': img, 'page': 'embedded'})
        return images

    def _to_gray(self, pil_img):
        img = np.array(pil_img)
        return cv2.cvtColor(img, cv2.COLOR_RGB2GRAY) if len(img.shape) == 3 else img

    def enhance_contrast(self, pil_img, clip_limit=3.0, tile_grid_size=(8,8)):
        img = self._to_gray(pil_img)
        clahe = cv2.createCLAHE(clipLimit=clip_limit, tileGridSize=tile_grid_size)
        return Image.fromarray(clahe.apply(img))

    def denoise(self, pil_img, h=15):
        img = self._to_gray(pil_img)
        return Image.fromarray(cv2.fastNlMeansDenoising(img, None, h, 7, 21))

    def remove_lines(self, pil_img, horz_len=40, vert_len=40):
        img = self._to_gray(pil_img)
        _, binary = cv2.threshold(img, 128, 255, cv2.THRESH_BINARY | cv2.THRESH_OTSU)
        horz_kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (horz_len, 1))
        vert_kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (1, vert_len))
        lines = cv2.bitwise_or(
            cv2.morphologyEx(binary, cv2.MORPH_OPEN, horz_kernel, iterations=1),
            cv2.morphologyEx(binary, cv2.MORPH_OPEN, vert_kernel, iterations=1)
        )
        cleaned = cv2.bitwise_and(binary, cv2.bitwise_not(lines))
        return Image.fromarray(cleaned)

    def correct_skew(self, pil_img):
        img = self._to_gray(pil_img)
        angle = determine_skew(img)
        if abs(angle) > 0.1:
            (h, w) = img.shape
            M = cv2.getRotationMatrix2D((w // 2, h // 2), angle, 1.0)
            rotated = cv2.warpAffine(img, M, (w, h), flags=cv2.INTER_CUBIC, borderMode=cv2.BORDER_REPLICATE)
            return Image.fromarray(rotated)
        return pil_img

    def invert_image(self, pil_img):
        return Image.fromarray(255 - np.array(pil_img))

    def compute_quality_metrics(self, pil_img):
        img = np.array(pil_img)
        return {"mean": np.mean(img), "std": np.std(img), "contrast": img.max() - img.min()}

    def auto_invert(self, pil_img):
        return np.mean(np.array(pil_img.convert('L'))) < 128

    def extract_layout_regions(self, pil_img):
        inputs = self.layout_processor(images=pil_img, return_tensors="pt")
        with torch.no_grad():
            outputs = self.layout_model(**inputs)
        target_sizes = torch.tensor([pil_img.size[::-1]])
        results = self.layout_processor.post_process_object_detection(
            outputs, threshold=0.5, target_sizes=target_sizes
        )[0]
        regions = []
        for box, label in zip(results["boxes"], results["labels"]):
            if label in [5, 8, 9]:  # Table, text, title
                xmin, ymin, xmax, ymax = map(int, box.tolist())
                regions.append((xmin, ymin, xmax, ymax, int(label)))
        return regions

    def crop_and_merge_regions(self, pil_img, regions):
        crops = [pil_img.crop((xmin, ymin, xmax, ymax)) for (xmin, ymin, xmax, ymax, _) in regions]
        if not crops:
            return pil_img
        widths, heights = zip(*(c.size for c in crops))
        merged = Image.new("RGB", (max(widths), sum(heights)), "white")
        y_offset = 0
        for c in crops:
            merged.paste(c, (0, y_offset))
            y_offset += c.size[1]
        return merged

    def binarize_by_sentence(self, pil_img):
        # Downscale image for OCR/drawing if very large
        max_dim = 1500
        scale = min(max_dim / pil_img.width, max_dim / pil_img.height, 1.0)
        img = pil_img
        if scale < 1.0:
            img = pil_img.resize((int(pil_img.width * scale), int(pil_img.height * scale)), Image.LANCZOS)

        data = pytesseract.image_to_data(img, output_type=pytesseract.Output.DICT)
        output = Image.new("RGB", img.size, "white")
        draw = ImageDraw.Draw(output)
        font_path = self.font_path

        lines = {}
        n_boxes = len(data['text'])
        for i in range(n_boxes):
            if int(data['conf'][i]) > 0 and data['text'][i].strip() != "":
                line_num = (data['block_num'][i], data['par_num'][i], data['line_num'][i])
                if line_num not in lines:
                    lines[line_num] = []
                lines[line_num].append(i)

        for line_indices in lines.values():
            lefts = [data['left'][i] for i in line_indices]
            tops = [data['top'][i] for i in line_indices]
            rights = [data['left'][i] + data['width'][i] for i in line_indices]
            bottoms = [data['top'][i] + data['height'][i] for i in line_indices]
            left = min(lefts)
            top = min(tops)
            right = max(rights)
            bottom = max(bottoms)
            box_width = right - left
            box_height = bottom - top
            sentence = " ".join([data['text'][i] for i in line_indices])

            # Maximize font size to fill the box
            font_size = max(int(box_height * 0.9), 18)
            if font_path:
                try:
                    font = ImageFont.truetype(font_path, font_size)
                except:
                    font = self.font
            else:
                font = self.font

            # Draw black rectangle
            draw.rectangle([left, top, right, bottom], fill="black")
            # Center text in box
            bbox = draw.textbbox((0, 0), sentence, font=font)
            text_width = bbox[2] - bbox[0]
            text_height = bbox[3] - bbox[1]
            x = left + max(0, (box_width - text_width) // 2)
            y = top + max(0, (box_height - text_height) // 2)
            draw.text((x, y), sentence, fill="white", font=font)

        # Upscale result to original image size
        if scale < 1.0:
            output = output.resize(pil_img.size, Image.LANCZOS)
        return output

    def process_single_page(
        self, img_dict,
        window_size=51, k=0.15,
        force_binarization="sentence",
        ocr_engine=None
    ):
        try:
            img = img_dict['image']
            page = img_dict['page']
            denoised = self.denoise(img)
            enhanced = self.enhance_contrast(denoised)
            metrics = self.compute_quality_metrics(enhanced)
            processed = self.binarize_by_sentence(enhanced)
            bin_method = "sentence"
            no_lines = self.remove_lines(processed)
            deskewed = self.correct_skew(no_lines)
            invert = self.auto_invert(deskewed)
            if invert:
                deskewed = self.invert_image(deskewed)
            regions = self.extract_layout_regions(deskewed.convert("RGB"))
            focused = self.crop_and_merge_regions(deskewed.convert("RGB"), regions)
            final_metrics = self.compute_quality_metrics(focused)
            ocr_results = {}
            if ocr_engine is not None:
                ocr_results["grayscale"] = ocr_engine(enhanced)
                ocr_results["binarized"] = ocr_engine(processed)
            return {
                "image": focused,
                "page": page,
                "metrics": final_metrics,
                "binarization": bin_method,
                "inverted": invert,
                "regions": regions,
                "ocr_results": ocr_results if ocr_results else None
            }
        except Exception as e:
            return {"image": None, "page": img_dict.get('page', '?'), "metrics": {}, "error": str(e)}

    def preprocess(
        self, input_path,
        window_size=51, k=0.15,
        force_binarization="sentence",
        ocr_engine=None
    ):
        images = self.normalize_document(input_path)
        # ThreadPoolExecutor works with instance methods and lambdas
        with concurrent.futures.ThreadPoolExecutor(max_workers=min(4, len(images))) as executor:
            results = list(
                executor.map(
                    lambda img_dict: self.process_single_page(
                        img_dict,
                        window_size=window_size,
                        k=k,
                        force_binarization=force_binarization,
                        ocr_engine=ocr_engine
                    ),
                    images
                )
            )
        results.sort(key=lambda x: (str(x['page'])))
        return results




















import streamlit as st
from agents.preprocess_agent import PreprocessAgent
from PIL import Image
import tempfile
import io
import zipfile

st.set_page_config(page_title="AI PreprocessAgent Demo", layout="wide")
st.title("AI-Powered Universal Document PreprocessAgent Demo")
st.markdown(
    """
    Upload a PDF, DOCX, TXT, or image file. The AI PreprocessAgent will automatically clean, enhance, deskew, segment (DETR), and prepare it for OCR/extraction, selecting the best methods for each document type.
    """
)

uploaded_file = st.file_uploader(
    "Upload a document (.pdf, .docx, .txt, .jpg, .jpeg, .png, .bmp, .tiff):",
    type=["pdf", "docx", "txt", "jpg", "jpeg", "png", "bmp", "tiff"]
)

if uploaded_file is not None:
    with tempfile.NamedTemporaryFile(delete=False, suffix="." + uploaded_file.name.split('.')[-1]) as temp_file:
        temp_file.write(uploaded_file.read())
        temp_path = temp_file.name

    st.info("Processing, please wait. All decisions are made automatically by the agent.")
    agent = PreprocessAgent()
    results = agent.preprocess(temp_path, force_binarization="sentence")

    for i, page in enumerate(results):
        col1, col2 = st.columns([3, 1])
        with col1:
            if page.get("image"):
                st.image(page["image"], caption=f"Preprocessed Page {page['page']}", use_container_width=True)
            else:
                st.error(f"Error on page {page['page']}: {page.get('error')}")
        with col2:
            st.markdown(f"**Page:** {page['page']}")
            if page.get("binarization"):
                st.markdown(f"**Binarization:** {page['binarization'].capitalize()}")
            if "inverted" in page:
                st.markdown(f"**Inverted:** {'Yes' if page['inverted'] else 'No'}")
            if page.get("metrics"):
                st.markdown("**Quality Metrics:**")
                for k, v in page["metrics"].items():
                    st.write(f"{k}: {v:.2f}")
            else:
                st.write("No metrics available.")
            if page.get("regions"):
                st.markdown(f"**Detected Layout Regions:** {len(page['regions'])}")
                for region in page["regions"]:
                    st.write(f"Label {region[4]} @ ({region[0]}, {region[1]}, {region[2]}, {region[3]})")

    st.success("Processing complete! Ready for OCR/Extraction.")

    # Download all processed images as a zip
    zip_buffer = io.BytesIO()
    with zipfile.ZipFile(zip_buffer, "w") as zf:
        for page in results:
            if page.get("image"):
                img_bytes = io.BytesIO()
                page["image"].save(img_bytes, format="PNG")
                zf.writestr(f"preprocessed_page_{page['page']}.png", img_bytes.getvalue())
    st.download_button(
        label="Download All Processed Pages as ZIP",
        data=zip_buffer.getvalue(),
        file_name="preprocessed_pages.zip",
        mime="application/zip"
    )




























import os
import logging
from fastapi import FastAPI, HTTPException, BackgroundTasks
from pydantic import BaseModel, Field
from typing import List
from huggingface_hub import login
from transformers import pipeline
import chromadb

# --- CONFIGURATION ---

os.environ["HUGGINGFACEHUB_API_TOKEN"] = "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"
login(token=os.environ["HUGGINGFACEHUB_API_TOKEN"])

chroma_client = chromadb.Client()
COLLECTION_NAME = "insurance_embeddings"
collection = chroma_client.get_or_create_collection(COLLECTION_NAME)

# Load Hugging Face transformers pipeline for embeddings
embedding_pipeline = pipeline("feature-extraction", model="thenlper/gte-small")

logging.basicConfig(level=logging.INFO, format='%(asctime)s %(levelname)s %(message)s')

# --- FASTAPI SETUP ---

app = FastAPI(title="Optimized Insurance Sync Agent API with ChromaDB Viewer")

class ValidatedInsuranceData(BaseModel):
    policy_id: str = Field(..., example="POL-123456")
    customer_name: str = Field(..., example="Jane Doe")
    claim_amount: float = Field(..., example=15000.0)
    description: str = Field(..., example="Water damage to basement")
    date_of_loss: str = Field(..., example="2025-06-01")
    status: str = Field(..., example="approved")

class ValidatedRecord(BaseModel):
    record_id: int = Field(..., example=1234)
    validated_json: ValidatedInsuranceData

class BatchValidatedRecords(BaseModel):
    records: List[ValidatedRecord]

# --- BATCH EMBEDDING GENERATION ---

def batch_generate_embeddings(records: List[ValidatedRecord]):
    texts = [rec.validated_json.json() for rec in records]
    embeddings = []
    for t in texts:
        # The output is [ [ [768 floats], ... ] ] for a single text (batch size 1, sequence length, hidden size)
        output = embedding_pipeline(t)
        # Average over the sequence dimension (tokens) to get a single vector
        vector = [float(sum(x)/len(x)) for x in zip(*output[0])]
        embeddings.append(vector)
    return embeddings

# --- BACKGROUND SYNC TASK ---

async def sync_records_to_chroma(records: List[ValidatedRecord], embeddings: List):
    ids = []
    metadatas = []
    documents = []
    failed_records = []

    for idx, record in enumerate(records):
        try:
            ids.append(str(record.record_id))
            metadatas.append({"record_id": record.record_id})
            documents.append(record.validated_json.json())
        except Exception as e:
            logging.error(f"Error preparing record {record.record_id}: {e}")
            failed_records.append(record.record_id)

    try:
        collection.add(
            ids=ids,
            embeddings=embeddings,
            metadatas=metadatas,
            documents=documents
        )
        logging.info(f"Synced batch of {len(ids)} records to ChromaDB via API.")
    except Exception as e:
        logging.error(f"Error syncing batch records to ChromaDB: {e}")
        # Retry logic or dead-letter queue could be added here
        for i, record_id in enumerate(ids):
            try:
                collection.add(
                    ids=[record_id],
                    embeddings=[embeddings[i]],
                    metadatas=[metadatas[i]],
                    documents=[documents[i]]
                )
                logging.info(f"Retried and synced record {record_id} to ChromaDB.")
            except Exception as ex:
                logging.error(f"Failed to sync record {record_id} after retry: {ex}")

# --- ASYNC BATCH ENDPOINT ---

@app.post("/sync_batch")
async def sync_batch_records(
    batch: BatchValidatedRecords,
    background_tasks: BackgroundTasks
):
    """
    Receives a batch of validated insurance data, generates embeddings, and stores them in ChromaDB asynchronously.
    """
    try:
        embeddings = batch_generate_embeddings(batch.records)
        background_tasks.add_task(sync_records_to_chroma, batch.records, embeddings)
        return {"status": "accepted", "synced_records": len(batch.records)}
    except Exception as e:
        logging.error(f"Error in batch sync request: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/health")
async def health_check():
    return {"status": "ok"}

# --- VIEW CHROMADB DATA ENDPOINT ---

@app.get("/view_data")
async def view_chromadb_data(limit: int = 20):
    """
    View all data stored in ChromaDB collection (documents, metadata, ids).
    """
    try:
        results = collection.get(include=["metadatas", "documents"], limit=limit)
        return results
    except Exception as e:
        logging.error(f"Error fetching data from ChromaDB: {e}")
        raise HTTPException(status_code=500, detail=str(e))

# To run: uvicorn agents.sync_agent:app --reload
